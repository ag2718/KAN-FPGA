{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KAN_Impl import KAN\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import mplhep as hep\n",
    "hep.style.use(\"CMS\")\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.cuda.empty_cache()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_range=[-8, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=torch.from_numpy(np.load('data/X_train_val.npy')).float().to(device)\n",
    "y_train=torch.from_numpy(np.load('data/y_train_val.npy')).float().to(device).argmax(dim=1)\n",
    "X_test=torch.from_numpy(np.load('data/X_test.npy')).float().to(device)\n",
    "y_test=torch.from_numpy(np.load('data/y_test.npy')).float().to(device).argmax(dim=1)\n",
    "\n",
    "# Create TensorDataset objects\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 64  # Adjust this based on your available memory\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = KAN([16,5,5], grid_size=30, spline_order=10, grid_eps=0.05, base_activation=nn.GELU, grid_range=[-8, 8], quantize=True, tp=7, fp=2, lut_res=128).to(device)\n",
    "# model = KAN([16,4,5], grid_size=30, spline_order=3, grid_eps=0.05, base_activation=nn.GELU, grid_range=grid_range).to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "model.to(device)\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "training_loss = []\n",
    "testing_loss = []\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(30):\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_train_loss = 0  # Initialize loss for the epoch\n",
    "    total_batches = 0\n",
    "    with tqdm(trainloader) as pbar:\n",
    "        for i, (inputs, labels) in enumerate(pbar):\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            # loss = criterion(output, labels.to(device)) + model.regularization_loss(regularize_activation=min(0.005 * epoch, 0.05), regularize_entropy=min(0.01 * epoch, 0.1), regularize_clipping=min(0.05 * epoch, 0.2))\n",
    "            loss = criterion(output, labels.to(device)) + model.regularization_loss(regularize_activation=0, regularize_entropy=min(0.01 * epoch, 0.1), regularize_clipping=min(0.05 * epoch, 0.2))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "            accuracy = (output.argmax(dim=1) == labels.to(device)).float().mean()\n",
    "\n",
    "            fracs_clipped = []\n",
    "            for layer in model.layers:\n",
    "                fracs_clipped.extend(round(x.item(), 4) for x in layer.get_frac_clipped())\n",
    "            \n",
    "            pbar.set_postfix(loss=loss.item(), accuracy=accuracy.item(), lr=optimizer.param_groups[0]['lr'], frac_clipped=fracs_clipped)\n",
    "    \n",
    "    average_train_loss = epoch_train_loss / total_batches\n",
    "    training_loss.append(average_train_loss)  # Record the average training loss\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            val_loss += criterion(output, labels.to(device)).item() + model.regularization_loss(regularize_activation=0, regularize_entropy=0, regularize_clipping=0.2)\n",
    "            val_accuracy += (\n",
    "                (output.argmax(dim=1) == labels.to(device)).float().mean().item()\n",
    "            )\n",
    "    val_loss /= len(testloader)\n",
    "    val_accuracy /= len(testloader)\n",
    "    testing_loss.append(val_loss)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    fracs_clipped = []\n",
    "    for layer in model.layers:\n",
    "        fracs_clipped.extend(x.item() for x in layer.get_frac_clipped())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}, Val Frac Clipped: {fracs_clipped}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid Range: \", grid_range)\n",
    "\n",
    "#Loop through each layer of the activation function\n",
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    print(\"Layer: \", i)\n",
    "\n",
    "    #Create dummy input\n",
    "    array = np.linspace(grid_range[0], grid_range[1], 256)\n",
    "    stacked_array = np.hstack([[array]*layer.in_features]).T\n",
    "    x=torch.from_numpy(stacked_array).float().to(device)\n",
    "\n",
    "    #Loop through each activation function\n",
    "    for j in range(layer.in_features):\n",
    "        for k in range(layer.out_features):\n",
    "            base_output = layer.base_activation(x)[:,j]*layer.base_weight[k,j]\n",
    "            spline_output = F.linear(layer.b_splines(x)[:,j,:], layer.scaled_spline_weight[k,j,:])\n",
    "\n",
    "            output = base_output + spline_output\n",
    "\n",
    "            print(output.cpu().detach().numpy().flatten().shape)\n",
    "            print(x.cpu().detach().numpy().shape)\n",
    "\n",
    "            plt.plot(x[:,j].cpu().detach().numpy(), output.cpu().detach().numpy(), label=f\"Layer {i}. Node {j} -> {k}\")\n",
    "            plt.xlabel(\"Input Value\")\n",
    "            plt.ylabel(\"Output Value\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid Range: \", grid_range)\n",
    "\n",
    "test_LUT = []\n",
    "\n",
    "#Loop through each layer of the activation function\n",
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    print(\"Layer: \", i)\n",
    "\n",
    "    #Create dummy input\n",
    "    array = np.linspace(grid_range[0], grid_range[1], 256)\n",
    "    stacked_array = np.hstack([[array]*layer.in_features]).T\n",
    "    x=torch.from_numpy(stacked_array).float().to(device)\n",
    "\n",
    "    #Loop through each activation function\n",
    "    for j in range(layer.in_features):\n",
    "        for k in range(layer.out_features):\n",
    "            base_output = layer.base_activation(x)[:,j]*layer.base_weight[k,j]\n",
    "            spline_output = F.linear(layer.b_splines(x)[:,j,:], layer.scaled_spline_weight[k,j,:])\n",
    "\n",
    "            output = base_output + spline_output\n",
    "\n",
    "            test_LUT.append(output.cpu().detach().numpy())\n",
    "\n",
    "            print(output.cpu().detach().numpy().flatten().shape)\n",
    "            print(x.cpu().detach().numpy().shape)\n",
    "\n",
    "            plt.plot(x[:,j].cpu().detach().numpy(), output.cpu().detach().numpy(), label=f\"Layer {i}. Node {j} -> {k}\")\n",
    "            plt.xlabel(\"Input Value\")\n",
    "            plt.ylabel(\"Output Value\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test_LUT[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
